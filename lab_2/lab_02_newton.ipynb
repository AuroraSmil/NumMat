{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2:  Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../styles/tma4215.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "\n",
    "# Comment out next line and execute this cell to restore the default notebook style \n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use Newton's method to find the roots to a *complex* function, namely the function\n",
    "    $$f_C(z) = z^3 - 1.$$\n",
    "### a)\n",
    "Write the complex function $f_C: \\mathbb{C} \\to \\mathbb{C}$ as a real vector-valued function $\\mathbf{f}: \\mathbb{R}^2 \\to \\mathbb{R}^2$ and write down the Jacobian.  \n",
    "(*Hint: use rectangular coordinates $z = x_1+\\mathrm{i}x_2$)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "   Solution: \n",
    "\n",
    "</font>\n",
    "\n",
    "Let $z = x_1 + i x_2$, then\n",
    "\\begin{align}\n",
    "f(z) &= (x_1 + i x_2)^3 - 1 \\\\\n",
    "&= x_{1}^{3} + 3 i x_{1}^{2} x_{2} - 3 x_{1} x_{2}^{2} - i x_{2}^{3} - 1 \\\\\n",
    "&= (x_{1}^{3} - 3 x_{1} x_{2}^{2} - 1 ) + i(3 x_{1}^{2} x_{2}- x_{2}^{3})\n",
    "\\end{align}\n",
    "Define a real vector-valued function $\\mathbf{f}: \\mathbb{R}^2 \\to \\mathbb{R}^2$, with $\\mathbf{x} = x_1 + ix_2$, then:\n",
    "\\begin{equation}\n",
    "\\mathbf{f}(\\mathbf{x}) = (x_{1}^{3} - 3 x_{1} x_{2}^{2} - 1 , 3 x_{1}^{2} x_{2}- x_{2}^{3})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jacobian is then \n",
    "\n",
    "$$\n",
    "d\\mathbf{f} = \n",
    "\\begin{pmatrix} \n",
    "3x_1^2 - 3x_2^2 &-6x_1 x_2 \\\\\n",
    "6x_1 x_2 & 3x_1^2 - 3x_2^2\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "Write a function $\\texttt{Newton}$ which performs Newton iteration until $\\|\\mathbf{f}(\\mathbf{x}^k)\\|_p$ is smaller than some given tolerance $tol$. You can choose what $p$-norm $\\|\\cdot\\|_p$ you use. The function should take as input parameters  \n",
    " - Initial guess $\\mathbf{x}^{(0)}$\n",
    " - tolerance $tol$\n",
    " - Maximum number of iterations $itermax$.  \n",
    "\n",
    "The function should return the final iterate $\\mathbf{x}^{(k)}$, and some indication of whether the iteration converged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as la\n",
    "\n",
    "\n",
    "def Newton(x0, tol, itermax, f, df, p=2):\n",
    "    x_k = x0\n",
    "    error = 2 * tol\n",
    "\n",
    "    iteration = 0\n",
    "    errors = []\n",
    "    while error > tol:\n",
    "        try:\n",
    "            x_k1 = x_k - la.inv(df(x_k)) @ f(x_k)\n",
    "        except la.LinAlgError:\n",
    "            return x_k, False\n",
    "        # error = la.norm(x_k - x_k1) # vi må prøve med en annen p-norm = [sum(abs(diff_i)**p)]**1/p\n",
    "        error = np.sum((x_k - x_k1)**p) ** (1 / p)\n",
    "        errors.append(error)\n",
    "\n",
    "        iteration += 1\n",
    "        x_k = x_k1\n",
    "\n",
    "    return x_k, True\n",
    "\n",
    "\n",
    "def jacobian(x):\n",
    "    x_1, x_2 = x\n",
    "    J = np.array([[3 * x_1 ** 2 - 3 * x_2 ** 2, - 6 * x_1 * x_2],\n",
    "                  [6 * x_1 * x_2, 3 * x_1 * 2 - 3 * x_2 ** 2]]).reshape((2, 2))\n",
    "    return J\n",
    "\n",
    "\n",
    "def func(x):\n",
    "    x_1, x_2 = x\n",
    "    return np.array([x_1 ** 3 - 3 * x_1 * x_2 ** 2 - 1, 3 * x_1 ** 2 * x_2 - x_2 ** 3]).reshape((-1, 1))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Test for different p-norms.\n",
    "\n",
    "x_0 = np.array([2, 1]).reshape(-1, 1)\n",
    "for p in [1, 2, 4, 10, 50, 100, 200, 10000]:\n",
    "    print(\"p =\", p)\n",
    "    res, conv = Newton(x_0, 1e-16, 100, func, jacobian, p)\n",
    "    print(res)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "The function $\\mathbf{f}$ has exactly three roots. (Which?) Hence, if the iteration converges it might converge to any of the three roots. You will now study the dependence on initial guess to which root the iteration converges. Pick $N$ equidistant values of $x^{(0)}_1$ in the interval $[-1,1]$ and $N$ equidistant values of $x^{(0)}_2$ in the interval $[-1,1]$. For each point $\\mathbf{x}^{(0)} = (x^{(0)}_1, x^{(0)}_2)^T$, perform the newton iteration you have defined above. Give the point a color depending on whether the iteration converged in time. If the iteration converged, the point should get a different color depending on which point it converged to. Plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "\n",
    "# r = 2\n",
    "x_1 = np.linspace(-1, 1, N)\n",
    "x_2 = np.linspace(-1, 1, N)\n",
    "tol = 1e-5\n",
    "itermax = 100\n",
    "\n",
    "fig, axes = plt.subplots()\n",
    "\n",
    "root_1 = np.array([1, 0]).reshape((-1, 1))\n",
    "root_2 = np.array([-.5, np.sqrt(3)/2]).reshape((-1, 1))\n",
    "root_3 = np.array([-.5, -np.sqrt(3)/2]).reshape((-1, 1))\n",
    "\n",
    "roots = [root_1, root_2, root_3]\n",
    "\n",
    "result = np.zeros((N, N))\n",
    "\n",
    "with_start_offset = True\n",
    "\n",
    "for i, v_1 in enumerate(x_1):\n",
    "    for j, v_2 in enumerate(x_2):\n",
    "        \n",
    "        offset = np.array([1/(4*N), 1/(4*N)]) if with_start_offset else 0\n",
    "        x = (np.array([i, j])  + offset).reshape((-1, 1))\n",
    "        # x = (np.array([i, j])).reshape((-1, 1))\n",
    "        x_k, converged = Newton(x, tol, itermax, func, jacobian)\n",
    "\n",
    "        if converged:\n",
    "            # result[j, i] = 1\n",
    "\n",
    "            if la.norm(x_k - root_1) < tol:\n",
    "                result[j, i] = 1\n",
    "            elif la.norm(x_k - root_2) < tol:\n",
    "                result[j, i ] = 2/3\n",
    "            elif la.norm(x_k - root_3) < tol:\n",
    "                result[j, i ] = 1/3\n",
    "        else:\n",
    "            result[j, i] = 0\n",
    "\n",
    "axes.imshow(result, interpolation='none', extent=(-1, 1, -1, 1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three roots of $\\mathbf{f}$ is $r_1 = 1$, $r_2 = -0.5 + i\\frac{\\sqrt{3}}{2}$ and $r_3 = -0.5 - i \\frac{\\sqrt{3}}{2}$.\n",
    "\n",
    "\n",
    "In the figure produced by the code above, the lightest three colours correspond to $r_1$, $r_2$ and $r_3$ respectively. If there are four colours in the plot, then the darkest colour corresponds to points that didn't converge. After adding a small perturbation to all starting points, all points seemed to converge. \n",
    "\n",
    "We see from the plot that almost all starting points end up converging to $r_1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint 1: You will encounter an error if you try with the initial guess $\\mathbf{x}^{(0)} = \\mathbf{0}$. It might be good to offset all initial guesses by some small perturbation $\\mathbf{\\delta}$.*     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint 2: A good way to measure which root the iteration converged to is looking at the argument if the point as a complex number, that is $$\\arg(x^{(k)}_1 + \\mathrm{i}x^{(k)}_2).$$ Store the result in a $N\\times N$ array. If the iteration did not converge, the point can be given the value* **None**. *The result can then be plotted using the matplotlib.pyplot function $\\texttt{imshow}$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "Discuss the following: Does it matter what $p$-norm you use in your Newton algorithm? What happens if you change norm? Does the result change qualitively? Remember that the $p$-norm on $\\mathbb{R}^n$  is defined as\n",
    "$$\n",
    "\\|\\mathbf{x}\\|_p := \\left(\\sum_{k=1}^n|x_k|^p\\right)^{1/p}\n",
    "$$\n",
    "for $1<p<\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "   Higher norms seem to make the algorithm converge faster but does not seem to reach the correct points.\n",
    "   \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) (Not mandatory)\n",
    "Modify your Newton function so that it returns the number of iterations used. Then, plot the number of iterations used for each initial guess $\\mathbf{x}^{(0)}.$ What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark\n",
    "If you colored the plot according to the hint, the white parts are called the *Julia set* of the rational function $$Q(z) = z - \\frac{f_C(z)}{f_C'(z)}.$$ The colored parts are called *Fatou components.* If you would like to more about the plot above and others like it, there is a link to the wikipedia page of [Julia sets](https://en.wikipedia.org/wiki/Julia_set)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
